{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "1#import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "#from lr_utils import load_dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split#to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-f5da797e279d>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-f5da797e279d>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    sklearn.datasets.\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "#since we cant use lr_utils we use train_test_split from sklearn\n",
    "#Also we need to load HDF5 dataset file, i think we can use pandas for the same\n",
    "file_name=(\"train_catvnoncat.h5\")\n",
    "#f=h5py.File('train_catvnoncat.h5','r') as hdf:\n",
    "#with h5py.File('train_catvnoncat.h5','r') as hdf:\n",
    "    \n",
    " #   ls=list(hdf.keys())\n",
    "#help(h5py.h5f.open)\n",
    "#train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "#pd.read_hdf('train_catvnoncat.h5')\n",
    "sklearn.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "    \n",
    "    \n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig,train_set_y,test_set_x_orig,test_set_y,classes=load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pwd\n",
    "#load breast cancer dataset\n",
    "cancer=load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer\n",
    "#splitting to training and test sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(cancer.data,cancer.target,stratify=cancer.target,random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2])=[0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "#Construncting model()\n",
    "\n",
    "#basic sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size\"\"\"\n",
    "    \n",
    "    s=1/(1+np.exp(-z))#finding the sigmoid\n",
    "    return s\n",
    "\n",
    "print(\"sigmoid([0,2])=\"+str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement parameter initialization, as vector of zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"this function creates a vector of zeros of shape (dim,1), for w and initialize b to 0\n",
    "    \n",
    "    Argument:\n",
    "    dim---size of w vector we want\n",
    "    (or number of parametersim this case)\n",
    "    \n",
    "    returns:\n",
    "    w-- initialized vector of shape(dim,1)\n",
    "    b--initialized scalar(corresponds to bias)\"\"\"\n",
    "    #rows-dim, columns=1\n",
    "    w=np.zeros((dim,1))\n",
    "    b=0\n",
    "    \n",
    "    assert(w.shape==(dim,1))\n",
    "    assert(isinstance(b,float)or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=[[0.]\n",
      " [0.]]\n",
      "b=0\n"
     ]
    }
   ],
   "source": [
    "dim=2\n",
    "w,b=initialize_with_zeros(dim)\n",
    "print(\"w=\"+str(w))\n",
    "print(\"b=\"+str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12780"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#INITIALIZATION\n",
    "w,b=initialize_with_zeros(X_train.shape[0]*X_train.shape[1])\n",
    "X_train.shape[0]\n",
    "#X_train[X_train.shape[0]*X_train.shape[1],1]\n",
    "#X_train=X_train.reshape(X_train.shape[0]*X_train.shape[1],1)\n",
    "#X_train.reshape(460,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORWARD AND BACKWARD PROPAGATION\n",
    "\n",
    "def propagate(w,b,X_train,y_train):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation \n",
    "    as in the equations explained\n",
    "    \n",
    "    Arguments:\n",
    "    w--------weights, a numpy array of size (num_px*num_px*3,1)\n",
    "    b---------bias scalar\n",
    "    X---------data of size(num_px*num_px,number of examples)\n",
    "    \n",
    "    Y- the label vector(containing 0 if non-cat(malignant),1 if cat(benign)) of size(1,number of examples)\n",
    "    \n",
    "    Returns :\n",
    "    cost------negative log likelihood cost for logistic regression\n",
    "    dw--------gradient of the  loss with respect to w, same shape as w\n",
    "    db--------gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips: write your code step by step for the propagation, use np.log, np.dot\n",
    "    \"\"\"\n",
    "    m=X_train.shape[1]#assigning the size variable m\n",
    "    \n",
    "    #FORWARD PROPAGATION(FROM X TO COST)\n",
    "    \n",
    "    #COMPUTE ACTIVATION\n",
    "    A=sigmoid(np.dot(w.T,X_train)+b)\n",
    "    \n",
    "    #Compute cost\n",
    "    \n",
    "    cost=(1/m)*(np.dot(y_train,np.log(A))+np.dot(1-y_train,np.log(1-A)))\n",
    "    \n",
    "    #gradient of loss with respect to w\n",
    "    dw=(1/m)*np.dot(X,(A-y_train).T)\n",
    "    #gradient of loss w.r.t. w\n",
    "    db=(1/m)*(A-y_train)\n",
    "    \n",
    "    assert(dw.shape==w.shape)\n",
    "    assert(db.dtype==float)\n",
    "    cost=np.sqeeze(cost)\n",
    "    assert(cost.shape==())\n",
    "    \n",
    "    grads={\"dw\":dw,\"db\":db}\n",
    "    \n",
    "    return grads,cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#grads,cost=propagate(w,b,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w,b,X_train,y_train,num_iterations,learning_rate,print_cost=False):\n",
    "    \"\"\"This function optimezes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w------------weights, a numpy array of size (num_px*num_px*3,1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\"\"\"\n",
    "    \n",
    "    costs=[]\n",
    "    for i in range(num_iterations):\n",
    "        #cost and gradient calculation(~ 1-4 lines of code)\n",
    "        grads,cost=propagate(w,b,X_train,y_train)\n",
    "        \n",
    "        #Retrive derivatives from  grads\n",
    "        \n",
    "        dw=grads[\"dw\"]\n",
    "        db=grads[\"db\"]\n",
    "        \n",
    "        #update rule{~ 2 lines of code}\n",
    "        #update the values with learning rate and gradient\n",
    "        w=w-learning_rate*dw\n",
    "        b=b-learning_rate*dw\n",
    "        \n",
    "        #Record the costs\n",
    "        if i % 100==0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "        #print the cost every 100 training iterations\n",
    "        \n",
    "        if print_cost and i%100==0:\n",
    "            print(\"cost after iteration % i: %f \"%(i,cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
